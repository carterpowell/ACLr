{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding the maximum page number...\n",
      "Maximum page number found: 63825\n",
      "Scraping data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2554/2554 [3:11:20<00:00,  4.50s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data scraping complete. Saved to basketball_data.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_max_page(start_date, end_date):\n",
    "    base_url = f\"https://www.prosportstransactions.com/basketball/Search/SearchResults.php?Player=&Team=&BeginDate={start_date}&EndDate={end_date}&ILChkBx=yes&InjuriesChkBx=yes&PersonalChkBx=yes&Submit=Search&start=0\"\n",
    "    response = requests.get(base_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    links = soup.find_all('a')\n",
    "    max_page = 0\n",
    "    for link in links:\n",
    "        href = link.get('href')\n",
    "        if href and 'start=' in href:\n",
    "            page_number = int(href.split('start=')[1])\n",
    "            if page_number > max_page:\n",
    "                max_page = page_number\n",
    "                \n",
    "    return max_page\n",
    "\n",
    "def scrape_data(start_date, end_date, max_page):\n",
    "    data = []\n",
    "    base_url = \"https://www.prosportstransactions.com/basketball/Search/SearchResults.php\"\n",
    "    params = {\n",
    "        \"Player\": \"\",\n",
    "        \"Team\": \"\",\n",
    "        \"BeginDate\": start_date,\n",
    "        \"EndDate\": end_date,\n",
    "        \"ILChkBx\": \"yes\",\n",
    "        \"InjuriesChkBx\": \"yes\",\n",
    "        \"PersonalChkBx\": \"yes\",\n",
    "        \"Submit\": \"Search\",\n",
    "        \"start\": 0\n",
    "    }\n",
    "\n",
    "    for start in tqdm(range(0, max_page + 1, 25)):\n",
    "        params[\"start\"] = start\n",
    "        response = requests.get(base_url, params=params)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        table = soup.select_one('.datatable')\n",
    "        \n",
    "        if table:\n",
    "            df = pd.read_html(str(table))[0]\n",
    "            data.append(df)\n",
    "    \n",
    "    return pd.concat(data, ignore_index=True)\n",
    "\n",
    "def main():\n",
    "    start_date = \"2000-01-01\"\n",
    "    end_date = \"2024-05-22\"\n",
    "    \n",
    "    print(\"Finding the maximum page number...\")\n",
    "    max_page = get_max_page(start_date, end_date)\n",
    "    print(f\"Maximum page number found: {max_page}\")\n",
    "    \n",
    "    print(\"Scraping data...\")\n",
    "    data = scrape_data(start_date, end_date, max_page)\n",
    "    \n",
    "    # Adjust the column names and remove the header row from the data\n",
    "    data.columns = data.iloc[0]\n",
    "    data = data.drop(0).reset_index(drop=True)\n",
    "    \n",
    "    # Save the data to a CSV file\n",
    "    data.to_csv(\"basketball_data.csv\", index=False)\n",
    "    print(\"Data scraping complete. Saved to basketball_data.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dots removed. Saved to basketball_data_no_dot.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the existing data\n",
    "data = pd.read_csv(\"basketball_data.csv\")\n",
    "\n",
    "# Remove the '• ' prefix from player names in the Acquired and Relinquished columns\n",
    "data['Acquired'] = data['Acquired'].str.replace('• ', '', regex=False)\n",
    "data['Relinquished'] = data['Relinquished'].str.replace('• ', '', regex=False)\n",
    "\n",
    "# Save the cleaned data to a new CSV file\n",
    "data.to_csv(\"basketball_data_no_dot.csv\", index=False)\n",
    "\n",
    "print(\"Dots removed. Saved to basketball_data_no_dot.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status column added. Saved to basketball_data_with_status.csv\n"
     ]
    }
   ],
   "source": [
    "# Load the existing data\n",
    "data = pd.read_csv(\"basketball_data_no_dot.csv\")\n",
    "\n",
    "# Create a new column \"Status\"\n",
    "data['Status'] = 'Unknown'  # Initialize with \"Unknown\" status\n",
    "\n",
    "# Update \"Status\" based on the presence of player names in \"Acquired\" and \"Relinquished\" columns\n",
    "data.loc[data['Relinquished'].notna(), 'Status'] = 'Relinquished'\n",
    "data.loc[data['Acquired'].notna(), 'Status'] = 'Acquired'\n",
    "\n",
    "# Save the updated data to a new CSV file\n",
    "data.to_csv(\"basketball_data_with_status.csv\", index=False)\n",
    "\n",
    "print(\"Status column added. Saved to basketball_data_with_status.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acquired and Relinquished columns consolidated. Saved to basketball_data_consolidated.csv\n"
     ]
    }
   ],
   "source": [
    "# Load the existing data\n",
    "data = pd.read_csv(\"basketball_data_with_status.csv\")\n",
    "\n",
    "# Consolidate \"Acquired\" and \"Relinquished\" columns into a single \"Player\" column\n",
    "data['Player'] = data['Acquired'].fillna(data['Relinquished'])\n",
    "\n",
    "# Drop the now redundant \"Acquired\" and \"Relinquished\" columns\n",
    "data.drop(columns=['Acquired', 'Relinquished'], inplace=True)\n",
    "\n",
    "# Save the updated data to a new CSV file\n",
    "data.to_csv(\"basketball_data_consolidated.csv\", index=False)\n",
    "\n",
    "print(\"Acquired and Relinquished columns consolidated. Saved to basketball_data_consolidated.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACL injury subset created. Saved to basketball_data_acl_subset.csv\n"
     ]
    }
   ],
   "source": [
    "# Load the existing data\n",
    "data = pd.read_csv(\"basketball_data_consolidated.csv\")\n",
    "\n",
    "# Filter the data to include only rows with phrases related to ACL injuries in the \"Notes\" column\n",
    "acl_subset = data[data['Notes'].str.contains('ACL|anterior cruciate ligament|Anterior Cruciate Ligament|Anterior cruciate ligament', case=False)]\n",
    "\n",
    "# Save the ACL injury subset data to a new CSV file\n",
    "acl_subset.to_csv(\"basketball_data_acl_subset.csv\", index=False)\n",
    "\n",
    "print(\"ACL injury subset created. Saved to basketball_data_acl_subset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique players subset created. Saved to unique_players_subset.csv\n"
     ]
    }
   ],
   "source": [
    "# Load the ACL injury subset data\n",
    "acl_subset = pd.read_csv(\"basketball_data_acl_subset.csv\")\n",
    "\n",
    "# Create a subset with unique players\n",
    "unique_players_subset = acl_subset.drop_duplicates(subset='Player')\n",
    "\n",
    "# Save the unique players subset data to a new CSV file\n",
    "unique_players_subset.to_csv(\"unique_players_subset.csv\", index=False)\n",
    "\n",
    "print(\"Unique players subset created. Saved to unique_players_subset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique players subset created. Saved to acl_players_full.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Load the basketball_data_with_status DataFrame\n",
    "data_with_status = pd.read_csv(\"basketball_data_consolidated.csv\")\n",
    "\n",
    "# Load the ACL injury subset data\n",
    "acl_subset = pd.read_csv(\"basketball_data_acl_subset.csv\")\n",
    "\n",
    "# Get unique player names from the ACL injury subset\n",
    "unique_players = acl_subset['Player'].unique()\n",
    "\n",
    "# Filter the DataFrame to include only rows for the unique players\n",
    "acl_players_full = data_with_status[data_with_status['Player'].isin(unique_players)]\n",
    "\n",
    "# Save the unique players subset data to a new CSV file\n",
    "acl_players_full.to_csv(\"acl_players_full.csv\", index=False)\n",
    "\n",
    "print(\"Unique players subset created. Saved to acl_players_full.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing players:   0%|          | 0/84 [00:00<?, ?it/s]ERROR:root:Game log table not found for Tom Gugliotta (https://www.basketball-reference.com/players/g/guglitom01/gamelog/2001)\n",
      "Processing players:   8%|▊         | 7/84 [00:13<02:29,  1.95s/it]ERROR:root:Game log table not found for Felipe Lopez (https://www.basketball-reference.com/players/l/lopezfe01/gamelog/2004)\n",
      "Processing players:  10%|▉         | 8/84 [00:14<02:26,  1.92s/it]ERROR:root:Game log table not found for Jarron Collins (https://www.basketball-reference.com/players/c/collija01/gamelog/2004)\n",
      "Processing players:  17%|█▋        | 14/84 [00:25<02:06,  1.81s/it]ERROR:root:Game log table not found for Alex Garcia (https://www.basketball-reference.com/players/g/garcial01/gamelog/2006)\n",
      "Processing players:  19%|█▉        | 16/84 [00:29<02:05,  1.85s/it]ERROR:root:Game log table not found for Nenê (https://www.basketball-reference.com/players/n/nenen01/gamelog/2007)\n",
      "Processing players:  30%|██▉       | 25/84 [00:47<01:55,  1.96s/it]ERROR:root:Game log table not found for Mickael Gelabale (https://www.basketball-reference.com/players/g/gelabmi01/gamelog/2009)\n",
      "Processing players:  31%|███       | 26/84 [00:48<01:49,  1.89s/it]ERROR:root:Game log table not found for Jason Smith (https://www.basketball-reference.com/players/s/smithja01/gamelog/2010)\n",
      "Processing players:  32%|███▏      | 27/84 [00:50<01:42,  1.79s/it]ERROR:root:Game log table not found for Jason Richards (https://www.basketball-reference.com/players/r/richa00/gamelog/2010)\n",
      "Processing players:  39%|███▉      | 33/84 [01:03<01:55,  2.26s/it]ERROR:root:Game log table not found for Kareem Rush (https://www.basketball-reference.com/players/r/rushka01/gamelog/2011)\n",
      "Processing players:  44%|████▍     | 37/84 [01:12<01:53,  2.41s/it]ERROR:root:Game log table not found for Gani Lawal (https://www.basketball-reference.com/players/l/lawalga01/gamelog/2012)\n",
      "Processing players:  46%|████▋     | 39/84 [01:16<01:37,  2.16s/it]ERROR:root:Game log table not found for Othyus Jeffers (https://www.basketball-reference.com/players/j/jeffeot01/gamelog/2012)\n",
      "Processing players:  52%|█████▏    | 44/84 [01:26<01:26,  2.17s/it]ERROR:root:Game log table not found for Baron Davis (https://www.basketball-reference.com/players/d/davisba01/gamelog/2013)\n",
      "Processing players:  68%|██████▊   | 57/84 [01:53<00:55,  2.07s/it]ERROR:root:Game log table not found for Chris Andersen (https://www.basketball-reference.com/players/a/anderch01/gamelog/2018)\n",
      "Processing players:  89%|████████▉ | 75/84 [02:33<00:26,  2.91s/it]ERROR:root:Game log table not found for Kira Lewis Jr. (https://www.basketball-reference.com/players/l/lewiske01/gamelog/2023)\n",
      "Processing players:  92%|█████████▏| 77/84 [02:37<00:16,  2.35s/it]ERROR:root:Game log table not found for Chris Smith (https://www.basketball-reference.com/players/s/smithch04/gamelog/2023)\n",
      "Processing players:  93%|█████████▎| 78/84 [02:39<00:12,  2.11s/it]ERROR:root:Game log table not found for E.J. Liddell (https://www.basketball-reference.com/players/l/liddel01/gamelog/2023)\n",
      "Processing players:  94%|█████████▍| 79/84 [02:40<00:09,  1.90s/it]ERROR:root:Game log table not found for Montrezl Harrell (https://www.basketball-reference.com/players/h/harremo01/gamelog/2024)\n",
      "Processing players:  96%|█████████▋| 81/84 [02:44<00:05,  1.89s/it]ERROR:root:Game log table not found for Jay Scrubb (https://www.basketball-reference.com/players/s/scrubja01/gamelog/2025)\n",
      "Processing players:  98%|█████████▊| 82/84 [02:45<00:03,  1.80s/it]ERROR:root:Game log table not found for Charles Bassey (https://www.basketball-reference.com/players/b/bassech01/gamelog/2025)\n",
      "Processing players:  99%|█████████▉| 83/84 [02:47<00:01,  1.77s/it]ERROR:root:Game log table not found for Saddiq Bey (https://www.basketball-reference.com/players/b/beysa01/gamelog/2025)\n",
      "Processing players: 100%|██████████| 84/84 [02:49<00:00,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script completed. Check 'players_return_dates.csv' for the results.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import time\n",
    "\n",
    "# Load the dataset\n",
    "dataset_path = 'test_acl.csv'  # Update this to the path of your dataset\n",
    "players_df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Define the player IDs manually for Basketball-Reference\n",
    "player_ids = {\n",
    "    \"Tom Gugliotta\": \"guglitom01\",\n",
    "    \"Bonzi Wells\": \"wellsbo01\",\n",
    "    \"Jamal Crawford\": \"crawfja01\",\n",
    "    \"Chris Crawford\": \"crawfch01\",\n",
    "    \"Al Harrington\": \"harrial01\",\n",
    "    \"Vitaly Potapenko\": \"potapvi01\",\n",
    "    \"Steven Hunter\": \"huntest01\",\n",
    "    \"Felipe Lopez\": \"lopezfe01\",\n",
    "    \"Jarron Collins\": \"collija01\",\n",
    "    \"Jared Jeffries\": \"jeffrja01\",\n",
    "    \"Marcus Fizer\": \"fizerma01\",\n",
    "    \"Obinna Ekezie\": \"ekeziob01\",\n",
    "    \"Pat Garrity\": \"garripa01\",\n",
    "    \"Ben Handlogten\": \"handlbe01\",\n",
    "    \"Alex Garcia\": \"garcial01\",\n",
    "    \"Willie Green\": \"greenwi01\",\n",
    "    \"Nenê\": \"nenen01\",\n",
    "    \"Robert Swift\": \"swiftro01\",\n",
    "    \"Stromile Swift\": \"swiftst01\",\n",
    "    \"Nenad Krstic\": \"krstine01\",\n",
    "    \"Tony Allen\": \"allento01\",\n",
    "    \"D.J. Mbenga\": \"mbengdj01\",\n",
    "    \"Shaun Livingston\": \"livinsh01\",\n",
    "    \"Adam Morrison\": \"morriad01\",\n",
    "    \"Paul Davis\": \"davispa01\",\n",
    "    \"Mickael Gelabale\": \"gelabmi01\",\n",
    "    \"Jason Smith\": \"smithja01\",\n",
    "    \"Jason Richards\": \"richa00\",\n",
    "    \"Mike Wilks\": \"wilksmi01\",\n",
    "    \"Corey Brewer\": \"breweco01\",\n",
    "    \"Michael Redd\": \"reddmi01\",\n",
    "    \"Al Jefferson\": \"jeffeal01\",\n",
    "    \"Leon Powe\": \"powele01\",\n",
    "    \"Kareem Rush\": \"rushka01\",\n",
    "    \"Josh Howard\": \"howarjo01\",\n",
    "    \"Kendrick Perkins\": \"perkike01\",\n",
    "    \"Jeff Ayres\": \"pendeje02\",\n",
    "    \"Gani Lawal\": \"lawalga01\",\n",
    "    \"David West\": \"westda01\",\n",
    "    \"Othyus Jeffers\": \"jeffeot01\",\n",
    "    \"Eric Maynor\": \"maynoer01\",\n",
    "    \"Ricky Rubio\": \"rubiori01\",\n",
    "    \"Derrick Rose\": \"rosede01\",\n",
    "    \"Iman Shumpert\": \"shumpim01\",\n",
    "    \"Baron Davis\": \"davisba01\",\n",
    "    \"Brandon Rush\": \"rushbr01\",\n",
    "    \"Lou Williams\": \"willilo02\",\n",
    "    \"Rajon Rondo\": \"rondora01\",\n",
    "    \"Leandro Barbosa\": \"barbole01\",\n",
    "    \"Danilo Gallinari\": \"gallida01\",\n",
    "    \"Nate Robinson\": \"robinna01\",\n",
    "    \"J.J. Hickson\": \"hicksjj01\",\n",
    "    \"Jabari Parker\": \"parkeja01\",\n",
    "    \"Kendall Marshall\": \"marshke01\",\n",
    "    \"Tony Wroten Jr.\": \"wroteto01\",\n",
    "    \"Dante Exum\": \"exumda01\",\n",
    "    \"Jarrett Jack\": \"jackja01\",\n",
    "    \"Chris Andersen\": \"anderch01\",\n",
    "    \"Zach LaVine\": \"lavinza01\",\n",
    "    \"Brandon Knight\": \"knighbr03\",\n",
    "    \"O.G. Anunoby\": \"anunoog01\",\n",
    "    \"Kristaps Porzingis\": \"porzikr01\",\n",
    "    \"Pau Gasol\": \"gasolpa01\",\n",
    "    \"Dejounte Murray\": \"murrade01\",\n",
    "    \"Klay Thompson\": \"thompkl01\",\n",
    "    \"DeMarcus Cousins\": \"couside01\",\n",
    "    \"Max Strus\": \"strusma01\",\n",
    "    \"Jeremy Lamb\": \"lambje01\",\n",
    "    \"Jonathan Isaac\": \"isaacjo01\",\n",
    "    \"Spencer Dinwiddie\": \"dinwisp01\",\n",
    "    \"Markelle Fultz\": \"fultzma01\",\n",
    "    \"Thomas Bryant\": \"bryanth01\",\n",
    "    \"Jamal Murray\": \"murraja01\",\n",
    "    \"Dario Saric\": \"saricda01\",\n",
    "    \"P.J. Dozier\": \"doziepj01\",\n",
    "    \"Kira Lewis Jr.\": \"lewiske01\",\n",
    "    \"Joe Ingles\": \"inglejo01\",\n",
    "    \"Chris Smith\": \"smithch04\",\n",
    "    \"E.J. Liddell\": \"liddel01\",\n",
    "    \"Montrezl Harrell\": \"harremo01\",\n",
    "    \"Vlatko Cancar\": \"cancavl01\",\n",
    "    \"Jay Scrubb\": \"scrubja01\",\n",
    "    \"Charles Bassey\": \"bassech01\",\n",
    "    \"Saddiq Bey\": \"beysa01\"\n",
    "}\n",
    "\n",
    "# Function to scrape game logs and find the return date\n",
    "def get_return_date(player, injury_date, injury_season):\n",
    "    injury_date = datetime.strptime(injury_date, \"%m/%d/%y\")\n",
    "    player_id = player_ids.get(player)\n",
    "    return_season = injury_season + 1\n",
    "    if not player_id:\n",
    "        logging.warning(f\"Player ID not found for {player}\")\n",
    "        return \"Player ID not found\"\n",
    "    \n",
    "    # Construct URL for the game log page\n",
    "    url = f\"https://www.basketball-reference.com/players/{player_id[0]}/{player_id}/gamelog/{return_season}\"\n",
    "    logging.info(f\"Fetching URL: {url}\")\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise HTTPError for bad responses (4xx and 5xx)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Error fetching URL for {player} ({url}): {e}\")\n",
    "        return \"URL not found\"\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    game_log_table = soup.find('table', {'id': 'pgl_basic'})\n",
    "    if not game_log_table:\n",
    "        logging.error(f\"Game log table not found for {player} ({url})\")\n",
    "        return \"Game log table not found\"\n",
    "    \n",
    "    rows = game_log_table.find_all('tr', class_=lambda x: x != 'thead')\n",
    "    for row in rows:\n",
    "        game_date_str = row.find('td', {'data-stat': 'date_game'})\n",
    "        if not game_date_str:\n",
    "            continue\n",
    "        game_date = datetime.strptime(game_date_str.text, \"%Y-%m-%d\")\n",
    "        if game_date > injury_date:\n",
    "            return game_date.strftime(\"%m/%d/%y\")\n",
    "    return \"Return date not found\"\n",
    "\n",
    "# Apply the function to each player with a progress bar\n",
    "results = []\n",
    "for _, row in tqdm(players_df.iterrows(), total=players_df.shape[0], desc=\"Processing players\"):\n",
    "    player = row['Player']\n",
    "    injury_date = row['Injury_Date']\n",
    "    injury_season = row['Injury_Season']\n",
    "    return_date = get_return_date(player, injury_date, injury_season)\n",
    "    results.append({\"Player\": player, \"Injury_Date\": injury_date, \"Return_Date\": return_date})\n",
    "    time.sleep(1)  # Delay to avoid overwhelming the server\n",
    "\n",
    "# Convert results to DataFrame and save to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('players_return_dates.csv', index=False)\n",
    "\n",
    "print(\"Script completed. Check 'players_return_dates.csv' for the results.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
